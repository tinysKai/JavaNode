#### 背景
以电商平台为例，当用户下单并支付后，系统需要修改订单的状态并且增加用户积分。  
由于系统采用的是微服务架构，分离出了支付服务、订单服务和积分服务，每个服务都有独立数据库做数据存储。  
当用户支付成功后，无论是修改订单状态失败还是增加积分失败，都会 造成数据的不一致。  

![fenbushi](https://github.com/tinysKai/JavaNote/blob/master/image/article/2018/0709/fenbushi1.png)
  
  
#### 二阶段提交协议
X/Open_DTP(Distributed_Transaction_Process)是一个分布式事务模型，此模型主要使用二阶段提交（2PC，Two-Phase-Commit）来保证分布式事务的完整性。  
在这个模型里面，有三个角色：  
+ AP：Application，应用程序，业务层。
+ RM：Resource_Manager，资源管理器，关系型数据库或支持XA接口（XA规范是X/Open组织定义的分布式事务规范）的组件。
+ TM： Transaction_Manager ，事务管理器，负责各个 RM 的提交和回滚。

当应用程序（AP）调用了事务管理器（TM）的提交方法时，事务的提交分为两个阶段实行。

>第一阶段（准备阶段）  
![fenbushi](https://github.com/tinysKai/JavaNote/blob/master/image/article/2018/0709/fenbushi2.png)

TM 通知所有参与事务的各个 RM，给每个 RM 发送 prepare 消息。

RM 接收到消息后进入准备阶段后，要么直接返回失败，要么创建并执行本地事务，写本地事务日志（redo 和 undo 日志），但是 不提交（此处只保留最后一步耗时最少的提交操作给第二阶段执行）。

>第二阶段（提交 / 回滚阶段）  
![fenbushi](https://github.com/tinysKai/JavaNote/blob/master/image/article/2018/0709/fenbushi3.png)  

TM 收到 RM 准备阶段的失败消息或者获取 RM 返回消息超时，则直接给 RM 发送回滚（rollback）消息，否则发送提交（commit）消息。

RM 根据 TM 的指令执行提交或者回滚，执行完成后释放所有事务处理过程中使用的锁（最后阶段释放锁）。

>缺点  
+ TM通过XA接口与各个RM之间进行数据交互，从第一阶段的准备阶段，业务所涉及的数据就被锁定，并且锁定跨越整个提交流程。在高并发和涉及业务模块较多的情况下 对数据库的性能影响较大。
+ 二阶段是 反可伸缩模式 的，业务规模越大，涉及模块越多，局限性越大，系统可伸缩性越差。
+ 在技术栈比较杂的分布式应用中，存储组件有很多 不支持 XA 协议。

二阶段的诸多弊端，导致分布式系统下无法直接使用此方案来解决数据一致性问题，但它提供了解决分布式系统下数据一致性问题的思路。

#### 可靠消息最终一致性
可靠消息最终一致性方案本质上是 利用 MQ 组件实现的二阶段提交。此方案涉及 3 个模块：
+ 上游应用，执行业务并发送 MQ 消息。
+ 可靠消息服务和 MQ 消息组件，协调上下游消息的传递，并确保上下游数据的一致性。
+ 下游应用，监听 MQ 的消息并执行自身业务。

![fenbushi](https://github.com/tinysKai/JavaNote/blob/master/image/article/2018/0709/fenbushi4.png)  

>上游应用执行业务并发送 MQ 消息（第一阶段）  

上游应用将本地业务执行和消息发送绑定在同一个本地事务中，保证要么本地操作成功并发送 MQ 消息，要么两步操作都失败并回滚。

上游应用和可靠消息之间的业务交互图如下：  

![fenbushi](https://github.com/tinysKai/JavaNote/blob/master/image/article/2018/0709/fenbushi5.png)  

    1.上游应用发送待确认消息到可靠消息系统
    2.可靠消息系统保存待确认消息并返回
    3.上游应用执行本地业务
    4.上游应用通知可靠消息系统确认业务已执行并发送消息。
    5.可靠消息系统修改消息状态为发送状态并将消息投递到 MQ 中间件。

>下游应用监听 MQ 消息并执行业务（第二阶段）    

下游应用监听 MQ 消息并执行业务，并且将消息的消费结果通知可靠消息服务。

可靠消息的状态需要和下游应用的业务执行保持一致，可靠消息状态不是已完成时，确保下游应用未执行，可靠消息状态是已完成时，确保下游应用已执行。

下游应用和可靠消息服务之间的交互图如下：  
![fenbushi](https://github.com/tinysKai/JavaNote/blob/master/image/article/2018/0709/fenbushi6.png)  
  
    1.下游应用监听 MQ 消息组件并获取消息
    2.下游应用根据 MQ 消息体信息处理本地业务
    3.下游应用向 MQ 组件自动发送 ACK 确认消息被消费
    4.下游应用通知可靠消息系统消息被成功消费，可靠消息将该消息状态更改为已完成。  


通过分析以上两个阶段可能失败的情况，为了确保上下游数据的最终一致性，在可靠消息系统中，需要开发 `消息状态确认` 和 `消息重发` 两个功能以实现 BASE 理论的 Eventually Consistent 特性。


>消息状态确认  

可靠消息服务定时监听消息的状态，如果存在状态为待确认并且超时的消息，则表示上游应用和可靠消息交互中的步骤 4 或者 5 出现异常。  
  
可靠消息则携带消息体内的信息向上游应用发起请求查询该业务是否已执行。  
上游应用提供一个可查询接口供可靠消息追溯业务执行状态，如果业务执行成功则更改消息状态为已发送，否则删除此消息确保数据一致。具体流程如下：  

![fenbushi](https://github.com/tinysKai/JavaNote/blob/master/image/article/2018/0709/fenbushi7.png)  

     1.可靠消息查询超时的待确认状态的消息
     2.向上游应用查询业务执行的情况
     3.业务未执行，则删除该消息，保证业务和可靠消息服务的一致性。业务已执行，则修改消息状态为已发送，并发送消息到 MQ 组件。
 
  
>消息重发  

消息已发送则表示上游应用已经执行，接下来则确保下游应用也能正常执行。  


可靠消息服务发现可靠消息服务中存在消息状态为已发送并且超时的消息，则表示可靠消息服务和下游应用中存在异常的步骤，无论哪个步骤出现异常，可靠消息服务都将此消息重新投递到 MQ 组件中供下游应用监听。  


下游应用监听到此消息后，在保证幂等性的情况下重新执行业务并通知可靠消息服务此消息已经成功消费，最终确保上游应用、下游应用的数据最终一致性。具体流程如下：  

![fenbushi](https://github.com/tinysKai/JavaNote/blob/master/image/article/2018/0709/fenbushi8.png)  
  
    1.可靠消息服务定时查询状态为已发送并超时的消息
    2.可靠消息将消息重新投递到 MQ 组件中
    3.下游应用监听消息，在满足幂等性的条件下，重新执行业务。
    4.下游应用通知可靠消息服务该消息已经成功消费。
      
      
通过`消息状态确认`和`消息重发`两个功能，可以确保上游应用、可靠消息服务和下游应用数据的最终一致性。

当然在实际接入过程中，需要引入 人工干预 功能。比如引入重发次数限制，超过重发次数限制的将消息修改为死亡消息，等待人工干预。      


代入开篇案例，通过可靠消息最终一致性方案，第一阶段，订单状态更改之前，订单服务向可靠消息服务请求保存待确认消息。可靠消息服务保存消息并返回。

订单服务接收到返回信息后执行本地业务并通知可靠消息服务业务已执行。消息服务更改消息状态并将消息投递到 MQ 中间件。

第二阶段，积分系统监听到 MQ 消息，查看积分是否已增加，如果没有增加则修改积分，然后请求可靠消息服务。可靠消息服务接收到积分系统的请求，将消息状态更改为已完成。  


#### TCC（Try-Confirm-Cancel）
TCC 方案是二阶段提交的 另一种实现方式，它涉及 3 个模块，主业务、从业务 和 活动管理器（协作者）。

下面这张图是互联网上关于 TCC 比较经典的图示：  

![fenbushi](https://github.com/tinysKai/JavaNote/blob/master/image/article/2018/0709/fenbushi9.png)
  
第一阶段：主业务服务分别调用所有从业务服务的 try 操作，并在活动管理器中记录所有从业务服务。当所有从业务服务 try 成功或者某个从业务服务 try 失败时，进入第二阶段。

第二阶段：活动管理器根据第一阶段从业务服务的 try 结果来执行 confirm 或 cancel 操作。如果第一阶段所有从业务服务都 try 成功，则协作者调用所有从业务服务的 confirm 操作，否则，调用所有从业务服务的 cancel 操作。

在第二阶段中，confirm 和 cancel 同样存在失败情况，所以需要对这两种情况做 异常处理 以保证数据一致性。

+ Confirm 失败：则回滚所有 confirm 操作并执行 cancel 操作。
+ Cancel 失败：从业务服务需要提供自动 cancel 机制，以保证 cancel 成功。 
 
 
目前有很多基于 RPC 的 TCC 框架，但是不适用于微服务架构下基于 HTTP 协议的交互模式。我们这次只讨论基于 HTTP 协议的 TCC 实现。具体的实现流程如下：  

 ![fenbushi](https://github.com/tinysKai/JavaNote/blob/master/image/article/2018/0709/fenbushi10.png)  
 
 主业务服务调用从业务服务的 try 操作，并获取 confirm/cancel 接口和超时时间。
 
 如果从业务都 try 成功，主业务服务执行本地业务，并将获取的 confirm/cancel 接口发送给活动管理器，活动管理器会顺序调用从业务 1 和从业务 2 的 confirm 接口并记录请求状态，如果请求成功，则通知主业务服务提交本地事务。如果 confirm 部分失败，则活动管理器会顺序调用从业务 1 和从业务 2 的 cancel 接口来取消 try 的操作。
 
 如果从业务部分或全部 try 失败，则主业务直接回滚并结束，而 try 成功的从业务服务则通过定时任务来处理处于 try 完成但超时的数据，将这些数据做回滚处理保证主业务服务和从业务服务的数据一致。  
 
 
代入开篇提到的案例，通过 TCC 方案，订单服务在订单状态修改之前执行预增积分操作（try），并从积分服务获取 confirm/cancel 预增积分的请求地址。

如果预增积分（try）成功，则订单服务更改订单状态并通知活动管理器，活动管理器请求积分模块的 confirm 接口来增加积分。

如果预增积分（try）失败，则订单服务业务回滚。积分服务通过定时任务删除预增积分（try）超时的数据。

另外如果活动管理器调用积分服务的 confirm 接口失败，则活动管理器调用积分服务 cancel接口来取消预增积分，从而，保证订单和积分数据的最终一致性。 
 
 

*http://dwz.cn/6ysDSi*