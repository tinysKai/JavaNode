##  数据分布式一致性方案

### 数据一致性

系统拆分之后，不可避免的是数据也分散存储在各个服务化系统中，如何保证上下游业务关联数据的一致性就是需要考虑的问题。数据一致性保证需要分场景讨论，不同业务场景对数据一致性的时效、强度要求均不同，这里主要讨论订单及其上下游的数据一致性。

 

说明：这里的数据一致性为最终一致性，即柔性事务，当然中间过程在正常情况下会尽可能短暂，比如毫秒级。

#### 1、前提

订单系统对于所有写请求都保证幂等，比如下单、状态修改等，通过幂等表控制，幂等表跟订单表在同一个事务内操作。



#### 2、下单及订单更新（含状态变更）

###### 2.1 业务系统调用订单系统

业务系统调用订单系统OSP服务，有可能出现以下三种情况：

- 收到响应，为成功
- 收到响应，为失败
- 未收到响应/超时
  - 成功
  - 失败
  - OSP服务方未接收到请求

下面针对各种情况进行综合分析：

###### 2.2 收到响应，成功：

这种情况最简单，不需要额外处理，调用方直接处理成功结果即可。

###### 2.3 收到响应，失败：

失败情况分两种：需要重试、不需要重试

1. 需要重试，重试由调用方发起，是否重试由响应错误码决定；比如服务方提示，当前并发处理失败，需要重试保证最终处理成功。具体错误码需要根据业务场景详细制定。此时订单系统（服务提供方）需要保证幂等数据已回滚，调用方重试的srcId（防重发随机数）需要重新生成。
2. 不需要重试，比如直接返回DB操作失败，请求重放等错误码。调用方需要处理错误响应结果。



###### 2.4 未收到响应/超时

这种情况是最复杂的，可能是调用方发出的请求未到订单系统，也可能是订单系统处理超时，或者是订单系统处理返回时网络断开，此时需要分情况讨论、处理。

1. 首先对于调用方开说，现象是都是一样的，这里都需要重试，即将本次超时的请求重放，不变化其中任何数据；至于重试的具体技术方案会在后面详细展开。

2. 而对于服务提供方来说，处理起来就比较复杂了，分为以下几种情况：

   - 请求未收到，此时接收到请求数据后，直接作为全新请求数据处理，不需要额外做什么

   - 首次请求已收到，且正在处理中，幂等数据尚未提交事务，查不到。第二次请求过来，在防重放的幂等控制模块是无法拦截的，这里只能靠：

     - 新增数据的唯一ID（可为联合索引），严重依赖幂等表中唯一键。
     - 更新数据时的悲观锁，需要行级锁。

     否则没办法控制，所以这里也是服务端需要着重处理的场景。 

 3.订单系统返回时网络原因导致调用方未收到请求，此时在再次接收到重放的请求数据时，直接根据幂等判断条件返回成功即可。



#### 3、额度处理

不管是无忧贷、现金贷，还是花、唯分期，在订单处理结束后（放款、支付成功等），包括后续的还款成功后，都要尽快体现在额度变化上。而且这里的额度变化对于用户来说非常敏感，用户不希望看到已经放款额度没有减少，更不希望看到已还款，额度没有加回去。所以这块额度的变化需要在订单操作成功之后，由调用方继续**同步**调用额度相关服务（或者操作额度相关数据表），首次变更不走异步化操作，失败场景再考虑异步补偿。重试方式同样采用章节4中的技术方案。

#### 4、关于重试及补偿的技术方案

原则上，同步调用的重试统一应该由调用发起方处理，避免上游数据出现变化，而下游却按照原有请求数据重试导致的脏数据、数据不一致等问题。不过对于部分场景，可以自动利用osp的自动重试机制，用proxy来自动发起重试，减少调用方处理成本。

##### 4.1 重试实现的落地方案：

###### 4.1.1 调用发起方主动发起重试

以下三个场景需要调用发起方主动发起重试

- 如果调动方收到订单系统的响应，且错误码提示为需要重新发起重试
- 接收到订单系统发送的MQ消息，提示OSP自动重试失败，需要发起方再次处理
- 超时

此时需要调用发起方主动控制，提供两种思路：

1. 通过代码中直接重试，比如try{do someting….}catch(TimeoutException e){ }，这种做法比较笨重，不过胜在稳定
2. 通过spring Retry框架，直接在整个方法级别加上注解 @Retryable，目前一般在应用在springboot项目中，在普通的项目中需要额外的spring配置。详见：<https://github.com/spring-projects/spring-retry>

###### 4.1.2 对以上两种重试的补充方案：

关于重试失败后的处理，采用MQ本身的重试机制来处理，而不是直接落库，用saturn定时任务去扫描；以减少需要落地的数据库。具体流程如下：

![重试流程](https://s2.ax1x.com/2019/04/10/ATQ8MQ.png)

通过以上三种互为补充的重试策略，达到最终数据一致的目标，亦即最终完成分布式数据一致性的保证。 



#### 5、幂等控制方案调整

目前方案是通过幂等表和业务表在同一个事务中处理，利用DB的事务及唯一索引来保证数据不会重复插入，可靠性方面比较强，不过存在以下问题：

1. 大事务，由于业务处理可能会耗时较多，幂等表数据插入之后事务并未提交，如果此时相同请求再过来，另外的机器/线程处理时是不会报错的，只有最终事务提交时，两个数据产生冲突，才会有一个产生报错。极端情况下，如果业务处理耗时过长，调用方一直发起重试，会有多个请求同时在处理，DB层面会有多次回滚操作，对服务器性能也有较大影响。
2. 存在DB被脏流量攻击的可能性。由于数据直接透传到DB，中间并没有缓存层过滤，如果出现批量脏数据大量请求的场景，DB会被直接穿透攻击。

##### 5.1 替代方案一

幂等数据依然存入DB，但是进行事务拆分并维护幂等数据状态，具体实现如下：

1. 每次收到请求，订单系统组装幂等数据，存入幂等表，并设置状态为处理中，提交事务
2. 幂等数据存入成功后，订单系统开始处理正常业务流程。业务流程处理成功，落库DB，并提交事务。如果失败，也一样进入步骤3
3. 新起事务更新幂等数据状态，根据业务处理状态更新为成功/失败

 

替代方案一 解决了大事务的问题，减少了因为业务处理耗时过长，多次请求重放并行处理的问题，在幂等控制时就可以拦截掉。不过这种处理也存在问题：

1. 增加了对幂等操作的状态维护，复杂度有所提高，且幂等数据需要频繁更新，而不是像之前的只需要insert。
2. 一次操作会提交三次事务，可能存在三次DB操作状态不一致的情况，导致数据准确性受影响，需要额外考虑内部重试机制。
3. 依然没有解决DB直接被穿透攻击的问题。

##### 5.2 替代方案二

幂等数据不存入DB，而是放入redis缓存，利用redis的setnx指令特性确定数据是否幂等，类似于基于redis的分布式锁实现。关于setnx指令，可以参考 <http://redisdoc.com/string/setnx.html> 。 （后者使用hsetnx <http://redisdoc.com/hash/hsetnx.html>）

具体实现如下：

1. 每次收到请求，订单系统组装幂等数据，以biz_id+biz_type+scene_type+src_id为key，1为value存入redis（1 表示处理中，2表示已处理）
2. 存入成功后，订单系统开始处理正常业务逻辑，落库DB，并提交事务。
3. 更新redis中幂等数据的状态为已处理。（如果步骤2处理失败，同样需要更新幂等数据状态为处理失败）

替代方案二 解决了大事务、DB穿透攻击的问题，不过也存在以下问题：

1. 依然需要维护幂等数据的状态，且在redis中的数据会越积累越多，什么时候清理、清理规则都需要额外考虑
2. 由于存在至少两次操作redis、一次操作DB，这些操作之间依然可能存在部分失败的情况，导致数据准确性受影响，需要考虑内部重试机制。

#### 6、 补充问题-关于唯一性ID生成的统一维护

幂等处理时需要根据唯一业务ID（比如订单ID）作为主要的幂等条件（当然还需要其它条件，比如业务类型、业务场景、随机srcID等），这个在订单生成之后的更新场景，唯一性订单ID作为业务ID是没问题的。不过在下单时，由于订单尚未生成，这个唯一ID的选择就比较困难。

所以针对下单场景，在调用下单服务之前，先调用订单系统的ID生成服务获取一个唯一ID（需要传入userId），后续下单、更新包括失败重试等场景，都需要以这个唯一ID作为订单主ID。订单系统会校验ID的合法性（即是否为订单系统产生）。